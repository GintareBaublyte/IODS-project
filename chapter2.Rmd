# Regression and model validation

*Describe the work you have done this week and summarize your learning.*

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using  

## Data Structure

```{r, echo=FALSE} 
students2014 <- read.table("learning2014.txt")

```
The dataset that will be analysed shows that there are 166 observations and 7 variables. Gender is recorded by assigning 1 and 2 for "female" and "male" respectively. Age is recorded as the actual age of the respondents. Questions related to attitude, deep learning, strategic learning and surface learning are evaluated in Likert scale (from 1 to 5). Points are the sum of points that students received from every exam question.

```{r, echo=FALSE}
str(students2014)
```

The dimensions fuction also shows that there are 166 rows (observations) and 7 columns (variables).

```{r, echo=FALSE}
dim(students2014)
```


##Graphical Overview and Summaries of Variables


```{r, echo=FALSE}
library(ggplot2)
p1 <- ggplot(students2014, aes(x = attitude, y = points, col = gender))
p2 <- p1 + geom_point()
p3 <- p2 + geom_smooth(method = "lm")
p4 <- p3 + ggtitle("Student's attitude versus exam points")
p4


```


```{r}
pairs(students2014[-1], col = students2014$gender)

```


The gender variable shows that 110 students are female and 56 are male.

```{r, echo=FALSE}
summary(students2014$gender)

```

The average age of the students is 25.51 years old. The youngest respondent is 17 and the oldest 55 y.o. 

```{r, echo=FALSE}
summary(students2014$age)

```

The avarage value for attitude (which was measured in Likert scale (1-5)) is 3.143. Lowest value is 1.4 and the highest value is 5, which was also the highest possible value to choose from.

```{r, echo=FALSE}
summary(students2014$attitude)

```

The avarage value assigned to questions related to deep learning was 3.68. The smallest value was 1.583 and the maximum 4.917.

```{r, echo=FALSE}
summary(students2014$deep)

```

The avarage value of answers to questions related to strategic learning is 3.121. Minimum value is 1.25 and the biggest one 5.

```{r, echo=FALSE}
summary(students2014$stra)

```

The avarage value of answers to questions related to surface learning is 2.787. Minimum value is 1.583 and the biggest one 4.333.

```{r, echo=FALSE}
summary(students2014$surf)

```

The avarage total points that students scored in the exam were 22.72. The lowest score was 7 and the highest 33. The middle value (or the median) was 23, which is almost the same as the average.

```{r, echo=FALSE}
summary(students2014$points)

```


```{r, echo=FALSE}
library(GGally)
library(ggplot2)
p <- ggpairs(students2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p

```

##Regression Model

Now a regresion model will be chosen to explain the dependent variable, which is "points".

The first regretion model includes 3 variables to explain the dependant variable: 1) attitude, 2) surface learning, and 3) deep learning.

The statistical significance level (p value) for "attitude" is close to 0, which means that this variable is statistically significant in explaining the dependent variable. However, other two variables are statistically insignificant, thus, they need to be replaced.

```{r, echo=FALSE}
my_model <- lm(points ~ attitude + surf + deep, data = students2014)

summary(my_model)
```

The second regretion model includes 3 variables to explain the dependant variable: 1) attitude, 2) age, and 3) strategic learning.

The statistical significance level (p value) for "attitude" is close to 0, which means that this variable is statistically significant in explaining the dependent variable. However, other two variables are statistically insignificant (their values are more than 0.05), thus, they need to be replaced.


```{r, echo=FALSE}
my_model2 <- lm(points ~ attitude + age + stra, data = students2014)

summary(my_model2)
```

The third regretion model includes 3 variables to explain the dependant variable: 1) attitude, 2) age, and 3) strategic learning.

The statistical significance level (p value) for "attitude" is close to 0, which means that this variable is statistically significant in explaining the dependent variable. However, other two variables are statistically insignificant, thus, they need to be replaced.

After testing a few other variations I have decided to choose linear regresion model with just one explanatory variable "attitude", as only this variable was statistically significant in explaining the dependant variable.


```{r, echo=FALSE}
my_model3 <- lm(points ~ attitude, data=students2014)

summary(my_model3)
```

From the summary of the regression model it can be seen that the intercept point is at 11.64, which is the ?? in the model. The ?? is equal to 3.53 and it means that when points increases by 1, attitude increase by 3.53 if everything else remains the same. 

The multiple R-Squared shows how well the chosed regression model fits the data. In this case multiple R-squared is equal to 0.1906 which is really low and would mean that the model doesn't explain the given data well.


##Diagnostic Plots
###Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage

The assumptions for the model are:
- The errors are normally distributed (QQ plot)
- The errors are not correlated
- The errors have constant variance (Residuals vs Fitted)
- The size of the given error does not depend on the values of explanatory variables (Residuals vs Fitted)

The QQ plot shows reasonable fit of the model to the normality assumption because the points in the grapgh are distributed quite evenly along the line.

The "Residuals vs Fitted" grapgh can help see whether the constant variance assumption is true. Any pattern in the scatter plot implies a problem with the assumptions. In this case, the model doesn't seem to have a pattern so the model fit is reasonable.

Leverage measures how much impact a single observation has on the model. "Residuals vs leverage" plot can help identify those observations that have an unsually high impact. In this case, no single obervation stands out (also concidering that the scale is really small) so the model is quite well fitted.

```{r, echo=FALSE}
par(mfrow = c(2,2))
plot(my_model3, which = c(1:2, 5))



```

Even though R-sq wasn't high, the assumptions in the model were confirmed and linear regression seems to be quite a good fit to explain the dependant variable.



