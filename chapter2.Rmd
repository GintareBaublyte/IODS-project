# Regression and Model Validation

This week I learned to wrangle data. Data wrangling are steps that need to be taken in order to process the data into more convenient format for its further analysis. Some of such steps could include combination of similar questions into one variable, scaling of some values back to their original values.

After the data was ready to be analysed, I learnt how to analyse the structure of it and how to look deeper into the different variables (see the avarage values, min, max, etc.). Graphical exploration of data is an important part of understanding the information that is in the dataset. It allows to look at the variables in regards to other variables, see the corelations and other information. 

After getting a better picture of the variables that are in the dataset, I learnt how to choose an appropriate regression model to explain one dependant variable with one or multiple independant variables. Different information in the model summaries helps to indentify the best fitting model for the dataset. In addition to that, there is a number of diagnostic plots that can be utilized to explore the validity of model assumptions.

Overall, I learnt useful data analysis basics with R Studio.
  

## Data Structure

```{r, echo=FALSE} 
students2014 <- read.table("learning2014.txt")

```
The dataset that will be analysed shows that there are 166 observations and 7 variables. Gender is recorded by assigning 1 and 2 for "female" and "male" respectively. Age is recorded as the actual age of the respondents. Questions related to attitude, deep learning, strategic learning and surface learning are evaluated in Likert scale (from 1 to 5). Points are the sum of points that students received from every exam question.

```{r, echo=FALSE}
str(students2014)
```

The dimensions fuction also shows that there are 166 rows (observations) and 7 columns (variables).

```{r, echo=FALSE}
dim(students2014)
```


##Graphical Overview and Summaries of Variables

The grapgh bellow shows how students' attitude increases with higher points received in the exam. It seems that it's quite similar for both genders.

```{r, echo=FALSE}
library(ggplot2)
p1 <- ggplot(students2014, aes(x = attitude, y = points, col = gender))
p2 <- p1 + geom_point()
p3 <- p2 + geom_smooth(method = "lm")
p4 <- p3 + ggtitle("Student's attitude versus exam points")
p4


```




The gender variable shows that 110 students are female and 56 are male.

```{r, echo=FALSE}
summary(students2014$gender)

```

The average age of the students is 25.51 years old. The youngest respondent is 17 and the oldest 55 y.o. 

```{r, echo=FALSE}
summary(students2014$age)

```

The avarage value for attitude (which was measured in Likert scale (1-5)) is 3.143. Lowest value is 1.4 and the highest value is 5, which was also the highest possible value to choose from.

```{r, echo=FALSE}
summary(students2014$attitude)

```

The avarage value assigned to questions related to deep learning was 3.68. The smallest value was 1.583 and the maximum 4.917.

```{r, echo=FALSE}
summary(students2014$deep)

```

The avarage value of answers to questions related to strategic learning is 3.121. Minimum value is 1.25 and the biggest one 5.

```{r, echo=FALSE}
summary(students2014$stra)

```

The avarage value of answers to questions related to surface learning is 2.787. Minimum value is 1.583 and the biggest one 4.333.

```{r, echo=FALSE}
summary(students2014$surf)

```

The avarage total points that students scored in the exam were 22.72. The lowest score was 7 and the highest 33. The middle value (or the median) was 23, which is almost the same as the average.

```{r, echo=FALSE}
summary(students2014$points)

```

The grapgh bellow shows data on different variable pairs.

```{r, echo=FALSE}
library(GGally)
library(ggplot2)
p <- ggpairs(students2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p

```

##Regression Model

Now a regression model will be chosen to explain the dependent variable, which is "points".

The first regression model includes 3 variables to explain the dependant variable: 1) attitude, 2) surface learning, and 3) deep learning.

The statistical significance level (p value) for "attitude" is close to 0, which means that this variable is statistically significant in explaining the dependent variable. However, other two variables are statistically insignificant, thus, they need to be replaced.

```{r, echo=FALSE}
my_model <- lm(points ~ attitude + surf + deep, data = students2014)

summary(my_model)
```

The second regretion model includes 3 variables to explain the dependant variable: 1) attitude, 2) age, and 3) strategic learning.

The statistical significance level (p value) for "attitude" is close to 0, which means that this variable is statistically significant in explaining the dependent variable. However, other two variables are statistically insignificant (their values are more than 0.05), thus, they need to be replaced.


```{r, echo=FALSE}
my_model2 <- lm(points ~ attitude + age + stra, data = students2014)

summary(my_model2)
```


After testing a few other variations I have decided to choose linear regresion model with just one explanatory variable "attitude", as only this variable was statistically significant in explaining the dependant variable.


```{r, echo=FALSE}
my_model3 <- lm(points ~ attitude, data=students2014)

summary(my_model3)
```

From the summary of the regression model it can be seen that the intercept point is at 11.64, which is the alpha in the model. The beta is equal to 3.53 and it means that when attitude increases by 1, points increase by 3.53 if everything else remains the same. 

The multiple R-Squared shows how well the chosed regression model fits the data. In this case multiple R-squared is equal to 0.1906 which is really low and would mean that the model doesn't explain the given data well.


##Diagnostic Plots
###Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage

The assumptions for the model are:
- The errors are normally distributed (QQ plot)
- The errors are not correlated
- The errors have constant variance (Residuals vs Fitted)
- The size of the given error does not depend on the values of explanatory variables (Residuals vs Fitted)

The QQ plot shows reasonable fit of the model to the normality assumption because the points in the grapgh are distributed quite evenly along the line.

The "Residuals vs Fitted" grapgh can help see whether the constant variance assumption is true. Any pattern in the scatter plot implies a problem with the assumptions. In this case, the model doesn't seem to have a pattern so the model fit is reasonable.

Leverage measures how much impact a single observation has on the model. "Residuals vs leverage" plot can help identify those observations that have an unsually high impact. In this case, no single obervation stands out (also concidering that the scale is really small) so the model is quite well fitted.

```{r, echo=FALSE}
par(mfrow = c(2,2))
plot(my_model3, which = c(1:2, 5))



```

Even though R-sq wasn't high, the assumptions in the model were confirmed and linear regression seems to be quite a good fit to explain the dependant variable.



